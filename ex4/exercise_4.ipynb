{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j88AMjjYliG9"
   },
   "source": [
    "# Introduction\n",
    "Last week, you implemented a fully-connected neural network for image classification on the CIFAR10 Dataset and reached a test set accuracy of around ~0.5. This weeks task is to improve on that, by changing the model to a convolutional neural network that you'll build from scratch. Before starting on the programming exercise, we strongly recommend having watched the lecture and looked at the slides. All the information you need for solving this assignment is in this notebook, and all the code you will be implementing will take place within this notebook.\n",
    "\n",
    "CAUTION: You need python version >= 3.4!\n",
    "\n",
    "\n",
    "\n",
    "## Grading\n",
    "For this programming exercise, there are 100 points in total.\n",
    "\n",
    "**Required Exercises**\n",
    "\n",
    "| Section| Part                                         |Submitted Function                           | Points \n",
    "|--------|:-                                            |:-                                           | :-:    \n",
    "| 1      | [Implementing a Convolutional Neural Net](#convnet)                         |[`conv_forward_naive`](#conv_forward_naive)  | 15\n",
    "|        |                                              |[`max_pool_forward_naive`](#max_pool_forward_naive)| 10\n",
    "|        |                                              |[`conv_backward_naive`](#conv_backward_naive)| 15\n",
    "|        |                                              |[`max_pool_backward_naive`](#max_pool_backward_naive)| 10\n",
    "| 2      | [Three Layer Convolutional Neural Network](#ThreeLayerConvNet) | [`ThreeLayerConvNet`](#ThreeLayerConvNet)   | 15\n",
    "| 3      | [Qualitative Analysis](#qualitative_analysis)|[q1](#qualitative_analysis_questions)        | 15\n",
    "|        |                                              |[q2](#qualitative_analysis_questions)        | 20\n",
    "|        | Total Points                                 |                                             | 100\n",
    "\n",
    "\n",
    "## Image classification Benchmark CIFAR10\n",
    "Here are some quick facts again:\n",
    "The dataset consists of 50,000/10,000 training/test images (each in $32 \\times 32$ resolution with RGB color channels). We further split the training images into 45,000 training samples and 5,000 validation samples. Each pixel is represented as three floating point numbers, indicating the color intensity in the respective color channel. Each image is labeled with integers ranging from 0 to 9 (0-9), indicating the class of the image content. The 10 classes and according example images from the dataset are shown in the image below. \n",
    "\n",
    "![Cifar10](https://pytorch.org/tutorials/_images/cifar10.png)\n",
    "\n",
    "You can load the complete dataset into memory by just running the next cell. If you edited the function `get_CIFAR10_data` in exercise 3, then just reuse your code and copy it below. Dimension have to agree with:\n",
    "```python\n",
    "('X_train: ', (45000, 3, 32, 32))\n",
    "('y_train: ', (45000,))\n",
    "('X_val: ', (5000, 3, 32, 32))\n",
    "('y_val: ', (5000,))\n",
    "('X_test: ', (10000, 3, 32, 32))\n",
    "('y_test: ', (10000,))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "ZZGXLSZordIA",
    "outputId": "98c9e0ca-0fdf-48b9-81c8-7f3861f50ba8"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 data\n",
    "#Load utils\n",
    "from utils import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "# configure plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def get_CIFAR10_data(\n",
    "    num_training=45000, num_validation=5000, num_test=10000, subtract_mean=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for classifiers. These are the same steps as we used for the SVM, but\n",
    "    condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    (train, test) = cifar10.load_data()\n",
    "    X_train, y_train = train\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test, y_test = test\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    if subtract_mean:\n",
    "        mean_image = np.mean(X_train, axis=0)\n",
    "        X_train -= mean_image\n",
    "        X_val -= mean_image\n",
    "        X_test -= mean_image\n",
    "\n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "\n",
    "    # Package data into a dictionary\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": np.squeeze(y_train),\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": np.squeeze(y_val),\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": np.squeeze(y_test),\n",
    "    }\n",
    "\n",
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "    print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wdp9g2-ymd5A"
   },
   "source": [
    "# Convolutional Neural Nets\n",
    "As we saw in the previous exercise, Fully-Connected Neural Networks receive an input (a single vector), and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the “output layer” and in classification settings it represents the class scores. \n",
    "\n",
    "These Nets don’t scale well to full images. In CIFAR-10, images are only of size 32x32x3 (32 width, 32 hight, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32x32x3 = 3072 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200x200x3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly!\n",
    "\n",
    "Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a FC Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. Note that the word depth here refers to the third dimension of an activation volume or the number of filters, not to the depth of a full Neural Network, which can refer to the total number of layers in a network. For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions 32x32x3 (width, height, depth respectively). As we will soon see, the neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. Moreover, the final output layer for CIFAR-10 would have dimensions 1x1x10, because by the end of the ConvNet architecture, we will reduce the full image into a single vector of class scores, arranged along the depth dimension. \n",
    "\n",
    "## Layers used to build CNNs\n",
    "A ConvNet can be seen as a sequence of layers, and every layer of it transforms one volume of activations to another through a differentiable function. We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (the ones you built in exercise 3). We will stack these layers to form a full ConvNet architecture. In this tutorial the architecture will be as follows:\n",
    "\n",
    "- INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.\n",
    "- CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.\n",
    "- RELU layer will apply an elementwise activation function, such as the max(0,x)\n",
    "thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).\n",
    "- POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].\n",
    "- FC layers: these are just the fully connected layers you implemented in exercise 3. They flatten the input from 3D to 2D and then to affine transformations and activations. \n",
    "![CNN](https://i.imgur.com/1agcV6t.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNdYhTPof7Op"
   },
   "source": [
    "## Convolutional Layer\n",
    "\n",
    "The Conv layer is the core building block of a Convolutional Network that does most of the computational heavy lifting.\n",
    "\n",
    "The CONV layer consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels). During the forward pass, we slide (more precisely, convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network. Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map. We will stack these activation maps along the depth dimension and produce the output volume.\n",
    "\n",
    "*Local Connectivity*. When dealing with high-dimensional inputs such as images, as we saw above, it is impractical to connect neurons to all neurons in the previous volume. Instead, we will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron (equivalently this is the filter size). The extent of the connectivity along the depth axis is always equal to the depth of the input volume. It is important to emphasize again this asymmetry in how we treat the spatial dimensions (width and height) and the depth dimension: The connections are local in space (along width and height), but always full along the entire depth of the input volume.\n",
    "\n",
    "*Example 1*. For example, suppose that the input volume has size [32x32x3], (e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 5x5x3 = 75 weights (and +1 bias parameter). Notice that the extent of the connectivity along the depth axis must be 3, since this is the depth of the input volume.\n",
    "\n",
    "*Example 2*. Suppose an input volume had size [16x16x20]. Then using an example receptive field size of 3x3, every neuron in the Conv Layer would now have a total of 3x3x20 = 180 connections to the input volume +1 for the bias. Notice that, again, the connectivity is local in space (e.g. 3x3), but full along the input depth (20).\n",
    "\n",
    "## The following three hyperparameters control the output dimensions of a Conv Layer\n",
    "\n",
    "### Stride\n",
    "Stride denotes the 'stepsize' that the convolution operation uses to 'slide' the kernel over the input. Depending on the size of the stride, the size of the output volume differs. Using a stride 1 on a 7×7 input will lead to an output volume of size $5 \\times 5$. Whereas a stride of 2 applied on the same image will lead to an output volume of the size $3\\times 3$. The example below visualizes the difference. Note: The stride is constrained to integers that will lead to valid output volumes. In the example below a stride of 3 is invalid, since there won't be only valid dot products in the formulation. \n",
    "![Stride](https://i.imgur.com/CRTziQV.png)\n",
    "\n",
    "### Zero Padding\n",
    "As mentioned earlier, applying filters shrinks the height and width of the input image. Using padding is a possibility to maintain the size of the input volume when applying a filter to it. Zero-Padding adds zeros around the border of the input image. That way you can build deeper networks because your image won’t shrink when applying many layers and it is easier to match dimensions for other operations such as concatenations.It’s pretty straight forward to calculate the padding which you have to add to prevent the volumes from shrinking: $p=(f−1)/ 2$ with $f$ being the size of the filter-kernel. In the example below the filter-kernel has size $3\\times 3$, so a padding of $(3-1)/2=1$ at each spatial dimension of the input image will preserve it's spatial dimensions.  Additionally, more information from the pixels at the borders of the image will be used, since the filter will slide over it more than once. By the way, numpy offers the really convenient `np.pad()` function :)\n",
    "![Padding](https://i.imgur.com/ZyLyf7P.png)\n",
    "\n",
    "### Number of filters\n",
    "Each filter returns a single filter-map, therefore the number of fiters in a layer determine the depth dimension of the volume that gets passed on in the network.\n",
    "\n",
    "Example 3. Suppose an input volume had size $[16\\times 16 \\times 20]$ and a Conv Layer with 10 (alternatively 30) filters with kernel-size $3\\times 3$ and zero-padding of 1 (so spatial dimensions are preserved. Then the output volume will have dimensions $[16\\times 16 \\times 10]$ (alternatively $[16\\times 16 \\times 30]$).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"convnet\"></a>\n",
    "# 1. Implementing a Convolutional Neural Net\n",
    "\n",
    "In this exercise we want to implment a three layered convolutional neural network using a convolution layer with activation and max pooling. \n",
    "\n",
    "## 1.1 Forward pass\n",
    "\n",
    "Before you dive in make sure you understand the topic. Following sources might help you with that:\n",
    "- [Stanford cs231n Webpage about Convolutional Neural Networks](https://cs231n.github.io/convolutional-networks/)\n",
    "- [A blog post on Convolutions from different theoretical perspectives](https://timdettmers.com/2015/03/26/convolution-deep-learning/)\n",
    "\n",
    "\n",
    "## 1.1.1 Convolution Layer: forward\n",
    "\n",
    "Now please implement the function `conv_forward_naive` below, which calculates pre-activations $z^{(l)} = w^{(l-1)}*x + b^{(l-1)}$  and test your implementation with the provided test cases.\n",
    "\n",
    "Hint: Pad the input and then loop over all `N` samples in the batch, and over all Filters `F`, and over all spatial dimensions in `H_prime` and `W_prime` direction with step size `conv_param['stride']` to get the sub-regions of the image and calculate the dot products of the filter kernels and add the bias. Store the resulting values in the according spot in the output volume. \n",
    "\n",
    "<a id=\"conv_forward_naive\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7emza5V_nC0X"
   },
   "outputs": [],
   "source": [
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a convolutional layer.\n",
    "    The input consists of N data points, each with C channels, height H and\n",
    "    width W. We convolve each input with F different filters, where each filter\n",
    "    spans all C channels and has height HH and width WW.\n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - w: Filter weights of shape (F, C, HH, WW)\n",
    "    - b: Biases, of shape (F,)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input. \n",
    "        \n",
    "    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n",
    "    along the height and width axes of the input. Be careful not to modfiy the original\n",
    "    input x directly.\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    - cache: (x_pad, w, b, conv_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    # Extract shapes and constants\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = w.shape\n",
    "    stride = conv_param.get('stride', 1)\n",
    "    pad = conv_param.get('pad', 0)\n",
    "    # Check for parameter sanity\n",
    "    assert (H + 2 * pad - HH) % stride == 0, 'Sanity Check Status: Conv Layer Failed in Height'\n",
    "    assert (W + 2 * pad - WW) % stride == 0, 'Sanity Check Status: Conv Layer Failed in Width'\n",
    "    # Construct output placeholder\n",
    "    H_prime = 1 + (H + 2 * pad - HH) // stride\n",
    "    W_prime = 1 + (W + 2 * pad - WW) // stride\n",
    "    out = np.zeros((N, F, H_prime, W_prime))\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional forward pass.                         #\n",
    "    # Hint: you can use the function np.pad for padding.                      #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "   \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = (x_pad, w, b, conv_param)\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "rHAVUeG0tpbT",
    "outputId": "b3c0d33e-ebec-432d-b350-d00d23f43d03"
   },
   "outputs": [],
   "source": [
    "# Test the conv_forward_naive function\n",
    "\n",
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around e-8\n",
    "print('Testing conv_forward_naive')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecGU9qmURREp"
   },
   "source": [
    "### 1.1.2 Aside: Image processing via convolutions\n",
    "\n",
    "As fun way to both check your implementation and gain a better understanding of the type of operation that convolutional layers can perform, we will set up an input containing two images and manually set up filters that perform common image processing operations (grayscale conversion and edge detection). The convolution forward pass will apply these operations to each of the input images. We can then visualize the results as a sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "colab_type": "code",
    "id": "XSllneoGRa1s",
    "outputId": "9c5e09c9-6698-4980-ab8d-5b4b2b99ae55"
   },
   "outputs": [],
   "source": [
    "from PIL import Image  # run \"pip install pillow\" if you get an import error\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def read_img_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "kitten = read_img_from_url('https://i.imgur.com/tWRmyaa.jpg')\n",
    "puppy = read_img_from_url('https://i.imgur.com/Ib1WRcw.jpg')\n",
    "# kitten is wide, and puppy is already square\n",
    "d = kitten.size[1] - kitten.size[0]\n",
    "kitten = kitten.crop([-d//2,0,kitten.size[0]+d//2,kitten.size[1]])\n",
    "\n",
    "img_size = 200   # Make this smaller if it runs too slow\n",
    "puppy = np.array(puppy.resize((img_size, img_size)))\n",
    "kitten = np.array(kitten.resize((img_size, img_size)))\n",
    "x = np.zeros((2, 3, img_size, img_size))\n",
    "x[0, :, :, :] = puppy.transpose((2, 0, 1))\n",
    "x[1, :, :, :] = kitten.transpose((2, 0, 1))\n",
    "\n",
    "# Set up a convolutional weights holding 2 filters, each 3x3\n",
    "w = np.zeros((2, 3, 3, 3))\n",
    "\n",
    "# The first filter converts the image to grayscale.\n",
    "# Set up the red, green, and blue channels of the filter.\n",
    "w[0, 0, :, :] = [[0, 0, 0], [0, 0.3, 0], [0, 0, 0]]\n",
    "w[0, 1, :, :] = [[0, 0, 0], [0, 0.6, 0], [0, 0, 0]]\n",
    "w[0, 2, :, :] = [[0, 0, 0], [0, 0.1, 0], [0, 0, 0]]\n",
    "\n",
    "# Second filter detects horizontal edges in the blue channel.\n",
    "w[1, 2, :, :] = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]]\n",
    "\n",
    "# Vector of biases. We don't need any bias for the grayscale\n",
    "# filter, but for the edge detection filter we want to add 128\n",
    "# to each output so that nothing is negative.\n",
    "b = np.array([0, 128])\n",
    "\n",
    "# Compute the result of convolving each input in x with each filter in w,\n",
    "# offsetting by b, and storing the results in out.\n",
    "out, _ = conv_forward_naive(x, w, b, {'stride': 1, 'pad': 1})\n",
    "\n",
    "def imshow_noax(img, normalize=True, cmap=None):\n",
    "    \"\"\" Tiny helper to show images as uint8 and remove axis labels \"\"\"\n",
    "    if normalize:\n",
    "        img_max, img_min = np.max(img), np.min(img)\n",
    "        img = 255.0 * (img - img_min) / (img_max - img_min)\n",
    "    plt.imshow(img.astype('uint8'), cmap=cmap)\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "# Show the original images and the results of the conv operation\n",
    "plt.subplot(2, 3, 1)\n",
    "imshow_noax(puppy, normalize=False)\n",
    "plt.title('Original image')\n",
    "plt.subplot(2, 3, 2)\n",
    "imshow_noax(out[0, 0],cmap='gray')\n",
    "plt.title('Grayscale')\n",
    "plt.subplot(2, 3, 3)\n",
    "imshow_noax(out[0, 1],cmap='gray')\n",
    "plt.title('Edge Detection')\n",
    "plt.subplot(2, 3, 4)\n",
    "imshow_noax(kitten, normalize=False)\n",
    "plt.subplot(2, 3, 5)\n",
    "imshow_noax(out[1, 0],cmap='gray')\n",
    "plt.subplot(2, 3, 6)\n",
    "imshow_noax(out[1, 1],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1soIe3n9cM4J"
   },
   "source": [
    "## 1.1.3 Max-Pooling Layer: forward\n",
    "This layer applies the max function to sub regions of the input volume. These sub regions do usually not overlap. The max function takes the maximum value of each of the regions and copys it into the output volume, which has it's spatial dimensions reduced according to the size of the subregion. In the following example, the pooling layer is applied on 2x2 subregions of the input volume, reducing the width and height by 1/2 (or the resolution by 1/4) of its original size while the depth stays the same. For the upper left subregion, the max pooling function is calculated as follows: $p_{(1,1)} = max(a_{(1,1)},a_{(1,2)},a_{(2,1)},a_{(2,2)}) = max(3,5,7,9) = 9$. Hint: This can also be implemented like the nested loop for the convolution: a filter of size $2\\times 2$ with stride $2$ is used and it returns the maximum value for each subregion it slides over. \n",
    "![Max Pooling](https://i.imgur.com/ccxu58n.png)\n",
    "\n",
    "Now please implement the function `max_pool_forward_naive` below, which downsamples the image and test your implementation with the provided test cases.\n",
    "\n",
    "<a id=\"max_pool_forward_naive\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "etaqpXhVcMGM"
   },
   "outputs": [],
   "source": [
    "def max_pool_forward_naive(x, pool_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a max-pooling layer.\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C, H, W)\n",
    "    - pool_param: dictionary with the following keys:\n",
    "      - 'pool_height': The height of each pooling region\n",
    "      - 'pool_width': The width of each pooling region\n",
    "      - 'stride': The distance between adjacent pooling regions\n",
    "    No padding is necessary here. Output size is given by \n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, C, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H - pool_height) / stride\n",
    "      W' = 1 + (W - pool_width) / stride\n",
    "    - cache: (x, pool_param)\n",
    "    \"\"\"\n",
    "    # Extract shapes and constants\n",
    "    out = None\n",
    "    N, C, H, W = x.shape\n",
    "    HH = pool_param.get('pool_height', 2)\n",
    "    WW = pool_param.get('pool_width', 2)\n",
    "    stride = pool_param.get('stride', 2)\n",
    "    assert (H - HH) % stride == 0, 'Sanity Check Status: Max Pool Failed in Height'\n",
    "    assert (W - WW) % stride == 0, 'Sanity Check Status: Max Pool Failed in Width'\n",
    "    # Construct output placeholder\n",
    "    H_prime = 1 + (H - HH) // stride\n",
    "    W_prime = 1 + (W - WW) // stride\n",
    "    out = np.zeros((N, C, H_prime, W_prime))\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the max-pooling forward pass                            #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = (x, pool_param)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "XqzvA871c3cy",
    "outputId": "50e952f9-1c1a-460d-c9d2-b184744d0bfc"
   },
   "outputs": [],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Compare your output with ours. Difference should be on the order of e-8.\n",
    "print('Testing max_pool_forward_naive function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1fKMa-euLrR"
   },
   "source": [
    "## 1.2 Backward pass\n",
    "Last week you implemented the backpropagation of errors algorithm for affine layers and the ReLU activation function. Furthermore, in the pen&paper exercise, you explored what will happen to the gradient of a weight, when a weight is shared among inputs (Hint: the multivariable chain-rule applies). Below you see a simplified example of a single dot product in a convolution operation. Since our input $a^{(l-1)}$ has no depth dimension, only height $h$ and width $w$, the weight matrix of filter $j$ neither has depth. The filter size is $2\\times2$ and the dot product plus bias with the upper left corner of the input is computed as $z_{j,(1,1)}^{(l)} = w^{(l-1)}_{j} \\cdot a^{(l-1)}_{(1:2,1:2)} + b_j$. Then the ReLU activation function $a_{j,(1,1)}^{(l)} = max(0,z_{j,(1,1)}^{(l)})$ is applied. After the convolution and activation, the max-pooling operation is applied on a $2\\times 2$ subregion of the activation map $a_{j,(1:2,1:2)}^{(l)}$.\n",
    "![Conv Backprop](https://i.imgur.com/cfSzpXc.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INhrjy9AkVao"
   },
   "source": [
    "### 1.2.1 Convolution Layer: backward\n",
    "\n",
    "Again, this functions gets the output from `relu_backward` $\\frac{\\partial L_\\theta}{\\partial z^{(l)}}$ as the input, together with the cached variables from the forward pass $a^{(l-1)}$, $w^{(l-1)}$ and $b^{(l-1)}$ and the dict `conv_params`. It computes the derivatives `dw`$=\\frac{\\partial L_\\theta}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial w^{(l-1)}}$, `db`$=\\frac{\\partial L_\\theta}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial b^{(l-1)}}$ and `dx`$=\\frac{\\partial L_\\theta}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial a^{(l-1)}}$. All of these derivatives are the same as in last weeks exercise, the only difference is that we need the multivariable chain rule to calculate them because $z^{(l)}= w^{(l-1)}*a^{(l-1)} + b^{(l-1)}$ is the result of a convolution instead of a dot product $z^{(l)}= w^{(l-1)} a^{(l-1)} + b^{(l-1)}$ as in last weeks exercise.\n",
    "\n",
    "Now implement the `conv_backward_naive` function and test your implementation using numeric gradient checking.\n",
    "\n",
    "Implementation: loop again over all samples `N`, all filters `F`, and all image directions `H_prime` and `W_prime'` and sum the gradients from all respective regions where the filter was applied. \n",
    "<a id=\"conv_backward_naive\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8D1btZYFtueV"
   },
   "outputs": [],
   "source": [
    "def conv_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a convolutional layer.\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives.\n",
    "    - cache: A tuple of (x_pad, w, b, conv_param) as in conv_forward_naive\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x\n",
    "    - dw: Gradient with respect to w\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    dx, dw, db = None, None, None\n",
    "    # Extract shapes and constants\n",
    "    x_pad, w, b, conv_param = cache\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = w.shape\n",
    "    stride = conv_param.get('stride', 1)\n",
    "    pad = conv_param.get('pad', 0)\n",
    "    # Construct output placeholders\n",
    "    H_prime = 1 + (H + 2 * pad - HH) // stride\n",
    "    W_prime = 1 + (W + 2 * pad - WW) // stride\n",
    "    dx = np.zeros_like(x)\n",
    "    dw = np.zeros_like(w)\n",
    "    db = np.zeros_like(b)\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional backward pass.                        #\n",
    "    # Hint: dx needs to have the same shape as x                              #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "D-wlQRymtxHh",
    "outputId": "829bf96a-6636-4aa1-bc53-24bcb9dfcb49"
   },
   "outputs": [],
   "source": [
    "# Test the conv_backward_naive function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around e-8 or less.\n",
    "print('Testing conv_backward_naive function')\n",
    "print('dx error: ', rel_error(dx, dx_num))\n",
    "print('dw error: ', rel_error(dw, dw_num))\n",
    "print('db error: ', rel_error(db, db_num))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIzP0vQLflSb"
   },
   "source": [
    "### 1.2.2 Max-Pooling: backward\n",
    "\n",
    "You already saw in the forward pass, that the max-pooling operation can be impemented kind of like the convolution. The same applies to the gradient calculation. The gradient of the pooling function with respect to a single activation for a $2\\times 2$ pooling subregion is just $\\frac{p_{(1,1)}}{a_{(1,1)}} = \\begin{cases}\n",
    "1 \\text{, if } a_{(1,1)}>a_{(1,2)},a_{(2,1)},a_{(2,2)}\\\\\n",
    "0 \\text{, else }\\\\\n",
    "\\end{cases}$.\n",
    "Therefore the upstream derivative just gets multiplied by $1$ for the max location and by $0$ for all other locations. One possibility to solve this, is by following these steps:\n",
    "- loop again over all samples `N`, all channels `C`, and all image directions `H_prime'` and `W_prime'` and find the index of the maximum element in `x` and store the according value of `dout` in that location in `dx`.\n",
    "\n",
    "<a id=\"max_pool_backward_naive\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peMDdYg-f04s"
   },
   "outputs": [],
   "source": [
    "def max_pool_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a max-pooling layer.\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives\n",
    "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx = None\n",
    "    # Extract constants and shapes\n",
    "    x, pool_param = cache\n",
    "    N, C, H, W = x.shape\n",
    "    HH = pool_param.get('pool_height', 2)\n",
    "    WW = pool_param.get('pool_width', 2)\n",
    "    stride = pool_param.get('stride', 2)\n",
    "    H_prime = 1 + (H - HH) // stride\n",
    "    W_prime = 1 + (W - WW) // stride\n",
    "    # Construct output placeholder\n",
    "    dx = np.zeros_like(x)\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the max-pooling backward pass                           #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "RGVtaJArf5bC",
    "outputId": "e85ad2de-931c-4a6b-9100-8eab07e9cb2f"
   },
   "outputs": [],
   "source": [
    "# Test the max_pool_backward_naive function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = max_pool_forward_naive(x, pool_param)\n",
    "dx = max_pool_backward_naive(dout, cache)\n",
    "\n",
    "# Your error should be on the order of e-12\n",
    "print('Testing max_pool_backward_naive function:')\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3BDrQ-kk2xQ"
   },
   "source": [
    "## 1.3 Fast layers\n",
    "Below we provide a more sophisticated implementation of the above implemented convolution and max-pooling layers. They will decrease runtime for the following tasks and also allow students who didn't finish the tasks above to continue with the next exercises. If you want to know more about increasing efficiency of convolution operations, [here](https://sahnimanas.github.io/post/anatomy-of-a-high-performance-convolution/) is a neat ressource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "YXKO_X-rDH58"
   },
   "outputs": [],
   "source": [
    "from fast_layers import * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNM22Pg63b25"
   },
   "source": [
    "### 1.3.1 Convenience layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, conv layers are frequently followed by a ReLU nonlinearity and max pooling. To make these common patterns easy, we define convenience layers in the following. The convenience layers from exercise 3 are copied here too, because you need them for the implementation of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ll3pJpPf_d3g"
   },
   "outputs": [],
   "source": [
    "def conv_relu_pool_forward(x, w, b, conv_param, pool_param):\n",
    "    \"\"\"\n",
    "    Convenience layer that performs an convolution, followed by a ReLU and max pool\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the affine layer\n",
    "    - w, b: Weights for the affine layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    z, conv_cache = conv_forward_fast(x, w, b, conv_param)\n",
    "    a, relu_cache = relu_forward(z)\n",
    "    out, pool_cache = max_pool_forward_fast(a, pool_param)\n",
    "    cache = (conv_cache, relu_cache, pool_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def conv_relu_pool_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the conv-relu-max pool convenience layer\n",
    "    \"\"\"\n",
    "    conv_cache, relu_cache, pool_cache = cache\n",
    "    da = max_pool_backward_fast(dout, pool_cache)\n",
    "    dz = relu_backward(da, relu_cache)\n",
    "    dx, dw, db = conv_backward_fast(dz, conv_cache)\n",
    "    return dx, dw, db\n",
    "\n",
    "def affine_relu_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Convenience layer that performs an affine transform followed by a ReLU\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the affine layer\n",
    "    - w, b: Weights for the affine layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, fc_cache = affine_forward(x, w, b)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (fc_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def affine_relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the affine-relu convenience layer\n",
    "    \"\"\"\n",
    "    fc_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = affine_backward(da, fc_cache)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSqEC0Ed4ZF6"
   },
   "source": [
    "# 2. Three Layer Convolutional Neural Network\n",
    "In the next step, you'll assemble all of the above into a small Convolutional Neural Network. The input images are passed into a `conv_relu_pool_forward` layer and then an `affine_relu_forward` layer and another `affine_forward` layer. Then the `cross_entropy_loss` is applied on the predictions, which calculates the softmax scores inside, so you don't have to take care about that.\n",
    "Then you implement the backward pass via `affine_backward`, `affine_relu_backward` and `conv_relu_pool_backward`. The network structure should resemble the one in the image below. You don't have to care about flattening the activation volumes, since your `affine_forward` layer already takes care of this.\n",
    "\n",
    "![CNN](https://i.imgur.com/1agcV6t.png)\n",
    "<a id =\"ThreeLayerConvNet\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dP7rqIkZ4YUR"
   },
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(object):\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the following architecture:\n",
    "    conv - relu - 2x2 max pool - affine - relu - affine - softmax\n",
    "    The network operates on minibatches of data that have shape (N, C, H, W)\n",
    "    consisting of N images, each with height H and width W and with C input\n",
    "    channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_filters=32, filter_size=7,\n",
    "                 hidden_dim=100, num_classes=10, weight_scale=1e-3,\n",
    "                 dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "        Inputs:\n",
    "        - input_dim: Tuple (C, H, W) giving size of input data\n",
    "        - num_filters: Number of filters to use in the convolutional layer\n",
    "        - filter_size: Width/height of filters to use in the convolutional layer\n",
    "        - hidden_dim: Number of units to use in the fully-connected hidden layer\n",
    "        - num_classes: Number of scores to produce from the final affine layer.\n",
    "        - weight_scale: Scalar giving standard deviation for random initialization\n",
    "          of weights.\n",
    "        - reg: Scalar giving L2 regularization strength\n",
    "        - dtype: numpy datatype to use for computation.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.dtype = dtype\n",
    "        # initialize weights and biases\n",
    "        C, H, W = input_dim\n",
    "        HP, WP = 1 + (H - 2)//2, 1 + (W - 2)//2  # max pooling\n",
    "        self.params['W1'] = weight_scale * np.random.randn(num_filters, C, filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(num_filters)\n",
    "        self.params['W2'] = weight_scale * np.random.randn(num_filters*HP*WP, hidden_dim)\n",
    "        self.params['b2'] = np.zeros(hidden_dim)\n",
    "        self.params['W3'] = weight_scale * np.random.randn(hidden_dim, num_classes)\n",
    "        self.params['b3'] = np.zeros(num_classes)\n",
    "        \n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Evaluate loss and gradient for the three-layer convolutional network.\n",
    "        Input / output: Same API as TwoLayerNet in fc_net.py.\n",
    "        \"\"\"\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3']\n",
    "\n",
    "        # pass conv_param to the forward pass for the convolutional layer\n",
    "        # Padding and stride chosen to preserve the input spatial size\n",
    "        filter_size = W1.shape[2]\n",
    "        conv_param = {'stride': 1, 'pad': (filter_size - 1) // 2}\n",
    "\n",
    "        # pass pool_param to the forward pass for the max-pooling layer\n",
    "        pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "        scores = None\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the three-layer convolutional net,  #\n",
    "        # computing the class scores for X and storing them in the scores          #\n",
    "        # variable.                                                                #\n",
    "        #                                                                          #\n",
    "        # Remember you can use the functions defined in cs231n/fast_layers.py and  #\n",
    "        # cs231n/layer_utils.py in your implementation (already imported).         #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        \n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "\n",
    "        loss, softmax_grad = cross_entropy_loss(scores, y)\n",
    "        ############################################################################\n",
    "        # TODO: Implement the backward pass for the three-layer convolutional net, #\n",
    "        # storing the gradients in the grads variables. Make sure that grads[k]    #\n",
    "        # holds the gradients for self.params[k].                                  #\n",
    "        #                                                                          #\n",
    "        # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
    "        # automated tests.                                                         #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        # backpropagation of gradients\n",
    "\n",
    "        \n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "u5xUWHV7n-vj",
    "outputId": "f223846e-c415-4cb8-908d-486141e2981a"
   },
   "outputs": [],
   "source": [
    "model = ThreeLayerConvNet()\n",
    "\n",
    "# check forward pass\n",
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (no regularization): ', loss)\n",
    "\n",
    "# check gradients\n",
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "np.random.seed(231)\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerConvNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "# Errors should be small, but correct implementations may have\n",
    "# relative errors up to the order of e-2\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))\n",
    "\n",
    "print('Correct implementations may still have relative errors up to the order of e-2.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPhy3X7x1S2n"
   },
   "source": [
    "## 2.1 Overfit small data\n",
    "\n",
    "A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy. This is just a test, to see if the network has problems fitting the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "colab_type": "code",
    "id": "wLplg2Z61Njo",
    "outputId": "279ce53f-5978-4b9d-f745-cd6f2e349001"
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=15, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT EDIT THIS CELL ####\n",
    "solver.train()\n",
    "#### DO NOT EDIT THIS CELL ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5F66m3g58v3"
   },
   "source": [
    "## 2.2 Train the network on the full dataset\n",
    "This will take some time.. around 30-45 minutes. Make a short break and then continue. If you are using colab, please keep in mind that **colab resets your session after 30 mins of being idle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-PEGNeKI5_xl",
    "outputId": "38c8faa7-2be6-40d2-c3e7-8f11d0adc4d0"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "lr = 1e-3\n",
    "hidden_dim = 100\n",
    "epochs = 5\n",
    "# build the network\n",
    "model = ThreeLayerConvNet(weight_scale=1e-2,num_filters=32, hidden_dim=50)\n",
    "# optimize\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=epochs, batch_size=100,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### DO NOT EDIT THIS CELL ####\n",
    "solver.train()  \n",
    "solver.save_params(\"three_layer_conv_net.pkl\")  # please hand in this file as well!\n",
    "#### DO NOT EDIT THIS CELL ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "colab_type": "code",
    "id": "JmtjMwxRaQJm",
    "outputId": "2cfa237f-b56b-4235-f38d-fb2e93361fdb"
   },
   "outputs": [],
   "source": [
    "#@title Test set results\n",
    "#@markdown Visualization of the confusion matrix. On the vertical axis are the real labels,\n",
    "#@markdown on the horizontal axis are the predicted labels. \n",
    "#@markdown The overall accuracy of the model should be around ~0.57.\n",
    "\n",
    "acc, pred = solver.check_accuracy(data['X_test'], data['y_test'], num_samples=10000)\n",
    "y_pred = pred['y_pred']\n",
    "print('#########################################')\n",
    "print('Overall Accuracy on the test set: ', acc)\n",
    "print('#########################################')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap\n",
    "# sns heatmap because plt.matshow has bugs with setting ticklabels\n",
    "\n",
    "names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "conf_mat = confusion_matrix(y_true= data['y_test'], y_pred=y_pred)\n",
    "conf_mat = conf_mat/1000\n",
    "conf_mat = np.round(conf_mat,2)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "heatmap(conf_mat, cmap=\"Blues\", annot=True,annot_kws={\"size\": 10}, xticklabels=names, yticklabels=names)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-pmaAjOpe_b"
   },
   "source": [
    "### Browse the Test Set Predictions of your model\n",
    "Below you can insert different image indices `img_idx` and get the associated input image and a bar chart with ground truth and prediction. Your test set consists of 10,000 examples, so integers in the intervall $[0,9999]$ are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "id": "TOsDALr5X2gw",
    "outputId": "5e54ef58-52e8-421d-8688-24bfd3c628f1"
   },
   "outputs": [],
   "source": [
    "img_idx = 10\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def show_prediction(image_idx):\n",
    "    img = np.clip((np.transpose(data['X_test'][image_idx], [1,2,0])/255)+0.5,0,1)\n",
    "    scores = np.transpose(pred['scores'])\n",
    "    scores = np.round(softmax(scores[image_idx]),2)\n",
    "    gt = np.zeros(10)\n",
    "    gt[data['y_test'][image_idx]]=1\n",
    "\n",
    "    plt.figure(figsize = (2,2))\n",
    "    plt.imshow(img, interpolation='lanczos')\n",
    "    names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    x_axis = np.arange(len(names))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    width = 0.5 \n",
    "    rects1 = ax.bar(x_axis-width/2, gt, 0.5, label='Groundtruth')\n",
    "    rects1 = ax.bar(x_axis+width/2, scores, 0.5, label='Prediction')\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(11))\n",
    "    ax.set_xticklabels(['']+names+[''])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "show_prediction(img_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19WJS1Emzyul"
   },
   "source": [
    "# 3. Qualitative Analysis of Predictions\n",
    "<a id=\"qualitative_analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "colab_type": "code",
    "id": "hXTdqY3X1d1t",
    "outputId": "a8e704a7-4a58-4df5-8063-0bce33a68b68"
   },
   "outputs": [],
   "source": [
    "def prediction_diff(image_idx):\n",
    "    scores = np.transpose(pred['scores'])\n",
    "    scores = np.round(softmax(scores[image_idx]),2)\n",
    "    return 1 - scores[data['y_test'][image_idx]]\n",
    "\n",
    "differences = np.zeros(10000)\n",
    "for i in range(10000):\n",
    "    differences[i] = prediction_diff(i)\n",
    "\n",
    "worst = np.argpartition(differences, -50)[-50:]\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)  \n",
    "    img = np.clip((np.transpose(data['X_test'][worst[i]], [1,2,0])/255)+0.5,0,1)\n",
    "    plt.imshow(img, interpolation='lanczos')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "colab_type": "code",
    "id": "hXK2ZxxrXuXw",
    "outputId": "fa841f9e-34bc-4bc0-8e91-be19eabaae4e"
   },
   "outputs": [],
   "source": [
    "best = np.argpartition(differences, 50)[:50]\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)  \n",
    "    img = np.clip((np.transpose(data['X_test'][best[i]], [1,2,0])/255)+0.5,0,1)\n",
    "    plt.imshow(img, interpolation='lanczos')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bv9t_oCgX-3N",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in worst:\n",
    "    show_prediction(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAaFryE5YBWN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in best:\n",
    "    show_prediction(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qwh1DJJLg9BW"
   },
   "source": [
    "<a id=\"qualitative_analysis_questions\"></a>\n",
    "## 3.1 Qualitative Analysis of Predictions - Questions\n",
    "Once again, (1) analyze the predictions of your model. Do the false predictions share certain properties the true predictions don't exhibit? (2) You heard a lot about modifications for CNNs in the cs231n lecture. Describe and justify 3 modifications for CNNs, which could help to improve your model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwD0w8JVEcvv"
   },
   "source": [
    "(1) Your answere here\n",
    "\n",
    "(2) Your answere here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ex4_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlia",
   "language": "python",
   "name": "mlia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
